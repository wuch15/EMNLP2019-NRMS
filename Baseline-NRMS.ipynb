{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "import itertools\n",
    "import random "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('docs.tsv') as f:\n",
    "    newsdata=f.readlines()\n",
    "with open('train.tsv')as f:\n",
    "    trainuser=f.readlines()\n",
    "with open('valid.tsv')as f:\n",
    "    validuser=f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "news={}\n",
    "for line in newsdata:\n",
    "    linesplit=line.strip().split('\\t')\n",
    "    news[linesplit[0]]=word_tokenize(linesplit[3].lower())\n",
    "    \n",
    "newsindex={'NULL':0}\n",
    "for newsid in news:\n",
    "    newsindex[newsid]=len(newsindex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def newsample(array,ratio):\n",
    "    if ratio >len(array):\n",
    "        return random.sample(array*(ratio//len(array)+1),ratio)\n",
    "    else:\n",
    "        return random.sample(array,ratio)\n",
    "    \n",
    "npratio=4\n",
    "train_candidate=[]    \n",
    "train_label=[]\n",
    "train_user_his=[]\n",
    "\n",
    "for user in trainuser:\n",
    "    userline=user.replace('\\n','').split('\\t')\n",
    "    clickids=[newsindex[x.split('#TAB#')[0]] for x in userline[1].split('#N#')][-50:]\n",
    "    pdoc=[newsindex[x] for x in userline[2].split('#TAB#')[0].split()]\n",
    "    ndoc=[newsindex[x] for x in userline[2].split('#TAB#')[1].split()]\n",
    "    \n",
    "    for doc in pdoc:\n",
    "        negd=newsample(ndoc,npratio)\n",
    "        negd.append(doc)\n",
    "        candidate_label=[0]*npratio+[1]\n",
    "        candidate_order=list(range(npratio+1))\n",
    "        random.shuffle(candidate_order)\n",
    "        candidate_shuffle=[]\n",
    "        candidate_label_shuffle=[]\n",
    "        for i in candidate_order:\n",
    "            candidate_shuffle.append(negd[i])\n",
    "            candidate_label_shuffle.append(candidate_label[i])\n",
    "        train_candidate.append(candidate_shuffle)\n",
    "        train_label.append(candidate_label_shuffle)\n",
    "        train_user_his.append(clickids+[0]*(50-len(clickids))) \n",
    "        \n",
    "test_candidate=[] \n",
    "test_user_his=[]\n",
    "test_index=[]\n",
    "test_session_data=[]\n",
    "\n",
    "for user in validuser:\n",
    "    userline=user.replace('\\n','').split('\\t')\n",
    "    clickids=[newsindex[x.split('#TAB#')[0]] for x in userline[1].split('#N#')][-50:]\n",
    "    alldoc=[newsindex[x] for x in userline[2].split('#TAB#')[0].split()]\n",
    "    test_session_data.append([userline[0],userline[2].split('#TAB#')[0].split(),userline[2].split('#TAB#')[1]])\n",
    "    index=[]\n",
    "    index.append(len(test_candidate))\n",
    "                  \n",
    "    for doc in alldoc:\n",
    "        test_candidate.append(doc)\n",
    "        test_user_his.append(clickids+[0]*(50-len(clickids)))\n",
    "    index.append(len(test_candidate))\n",
    "    test_index.append(index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict={'PADDING':0}\n",
    "news_title=[[0]*30]\n",
    "\n",
    "for newsid in news:\n",
    "    title=[]\n",
    "    for word in news[newsid]:\n",
    "        if word not in word_dict:\n",
    "            word_dict[word]=len(word_dict)\n",
    "        title.append(word_dict[word])\n",
    "    title=title[:30]\n",
    "    news_title.append(title+[0]*(30-len(title)))\n",
    "\n",
    "news_title=np.array(news_title,dtype='int32') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_candidate=np.array(train_candidate,dtype='int32')\n",
    "train_label=np.array(train_label,dtype='int32')\n",
    "train_user_his=np.array(train_user_his,dtype='int32')\n",
    "\n",
    "test_candidate=np.array(test_candidate,dtype='int32') \n",
    "test_user_his=np.array(test_user_his,dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_batch_data_random(batch_size):\n",
    "    idlist = np.arange(len(train_label))\n",
    "    np.random.shuffle(idlist)\n",
    "    y=train_label\n",
    "    batches = [idlist[range(batch_size*i, min(len(y), batch_size*(i+1)))] for i in range(len(y)//batch_size+1)]\n",
    "    while (True):\n",
    "        for i in batches:\n",
    "            item = news_title[train_candidate[i]]\n",
    "            user=news_title[train_user_his[i]]\n",
    "            yield ([item,user], [y[i]])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_batch_data(batch_size):\n",
    "    idlist = np.arange(len(test_candidate))\n",
    "    batches = [idlist[range(batch_size*i, min(len(idlist), batch_size*(i+1)))] for i in range(len(idlist)//batch_size+1)]\n",
    "\n",
    "    while (True):\n",
    "        for i in batches:\n",
    "            item = news_title[test_candidate[i]]\n",
    "            user=news_title[test_user_his[i]]\n",
    "            yield ([item,user])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Attention(Layer):\n",
    "\n",
    "    def __init__(self, nb_head, size_per_head, **kwargs):\n",
    "        self.nb_head = nb_head\n",
    "        self.size_per_head = size_per_head\n",
    "        self.output_dim = nb_head*size_per_head\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.WQ = self.add_weight(name='WQ', \n",
    "                                  shape=(input_shape[0][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        self.WK = self.add_weight(name='WK', \n",
    "                                  shape=(input_shape[1][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        self.WV = self.add_weight(name='WV', \n",
    "                                  shape=(input_shape[2][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        super(Attention, self).build(input_shape)\n",
    "        \n",
    "    def Mask(self, inputs, seq_len, mode='mul'):\n",
    "        if seq_len == None:\n",
    "            return inputs\n",
    "        else:\n",
    "            mask = K.one_hot(seq_len[:,0], K.shape(inputs)[1])\n",
    "            mask = 1 - K.cumsum(mask, 1)\n",
    "            for _ in range(len(inputs.shape)-2):\n",
    "                mask = K.expand_dims(mask, 2)\n",
    "            if mode == 'mul':\n",
    "                return inputs * mask\n",
    "            if mode == 'add':\n",
    "                return inputs - (1 - mask) * 1e12\n",
    "                \n",
    "    def call(self, x):\n",
    "        if len(x) == 3:\n",
    "            Q_seq,K_seq,V_seq = x\n",
    "            Q_len,V_len = None,None\n",
    "        elif len(x) == 5:\n",
    "            Q_seq,K_seq,V_seq,Q_len,V_len = x\n",
    "        Q_seq = K.dot(Q_seq, self.WQ)\n",
    "        Q_seq = K.reshape(Q_seq, (-1, K.shape(Q_seq)[1], self.nb_head, self.size_per_head))\n",
    "        Q_seq = K.permute_dimensions(Q_seq, (0,2,1,3))\n",
    "        K_seq = K.dot(K_seq, self.WK)\n",
    "        K_seq = K.reshape(K_seq, (-1, K.shape(K_seq)[1], self.nb_head, self.size_per_head))\n",
    "        K_seq = K.permute_dimensions(K_seq, (0,2,1,3))\n",
    "        V_seq = K.dot(V_seq, self.WV)\n",
    "        V_seq = K.reshape(V_seq, (-1, K.shape(V_seq)[1], self.nb_head, self.size_per_head))\n",
    "        V_seq = K.permute_dimensions(V_seq, (0,2,1,3))\n",
    "        A = K.batch_dot(Q_seq, K_seq, axes=[3,3]) / self.size_per_head**0.5\n",
    "        A = K.permute_dimensions(A, (0,3,2,1))\n",
    "        A = self.Mask(A, V_len, 'add')\n",
    "        A = K.permute_dimensions(A, (0,3,2,1))    \n",
    "        A = K.softmax(A)\n",
    "        O_seq = K.batch_dot(A, V_seq, axes=[3,2])\n",
    "        O_seq = K.permute_dimensions(O_seq, (0,2,1,3))\n",
    "        O_seq = K.reshape(O_seq, (-1, K.shape(O_seq)[1], self.output_dim))\n",
    "        O_seq = self.Mask(O_seq, Q_len, 'mul')\n",
    "        return O_seq\n",
    "        \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0][0], input_shape[0][1], self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "MAX_SENT_LENGTH=30\n",
    "MAX_SENTS=50\n",
    "\n",
    "title_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32')\n",
    "embedding_layer = Embedding(len(word_dict), 300, trainable=True)\n",
    "embedded_sequences = embedding_layer(title_input)\n",
    "d_emb=Dropout(0.2)(embedded_sequences)\n",
    "selfatt=Attention(20,20)([d_emb,d_emb,d_emb])\n",
    "selfatt=Dropout(0.2)(selfatt)\n",
    "attention = Dense(200,activation='tanh')(selfatt)\n",
    "attention = Flatten()(Dense(1)(attention))\n",
    "attention_weight = Activation('softmax')(attention)\n",
    "rep=Dot((1, 1))([selfatt, attention_weight])\n",
    "titleEncoder = Model([title_input], rep)\n",
    "\n",
    "news_input = Input((MAX_SENTS,MAX_SENT_LENGTH,))\n",
    "news_encoders = TimeDistributed(titleEncoder)(news_input)\n",
    "news_encoders=Dropout(0.2)(Attention(20,20)([news_encoders,news_encoders,news_encoders]))\n",
    "candidates = keras.Input((1+npratio,MAX_SENT_LENGTH,))\n",
    "candidate_vecs = TimeDistributed(titleEncoder)(candidates)  \n",
    "news_attention= Dense(200,activation='tanh')(news_encoders)\n",
    "news_attention = Flatten()(Dense(1)(news_attention))\n",
    "news_attention_weight = Activation('softmax')(news_attention)\n",
    "userrep=Dot((1, 1))([news_encoders, news_attention_weight])\n",
    "logits = dot([userrep, candidate_vecs], axes=-1)\n",
    "logits = Activation(keras.activations.softmax)(logits)      \n",
    "model = Model([candidates,news_input], logits)\n",
    "model.compile(loss=['categorical_crossentropy'], optimizer='adam', metrics=['acc'])\n",
    "\n",
    "candidate_one = keras.Input((MAX_SENT_LENGTH,))\n",
    "candidate_one_vec = titleEncoder([candidate_one])\n",
    "score =Activation(keras.activations.sigmoid)(dot([userrep, candidate_one_vec], axes=-1))\n",
    "modeltest = keras.Model([candidate_one,news_input], score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ep in range(2):\n",
    "    traingen=generate_batch_data_random(30)\n",
    "    model.fit_generator(traingen, epochs=1,steps_per_epoch=len(train_label)//30)\n",
    "    \n",
    "valgen=generate_batch_data(1)\n",
    "pred = modeltest.predict_generator(valgen, steps=len(test_candidate),verbose=1)\n",
    "predictsession=[]\n",
    "for i in range(len(test_index)):\n",
    "    predictsession.append(pred[test_index[i][0]:test_index[i][1],0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import *\n",
    "with open('answer.json','w')as f:\n",
    "    for m in range(len(predictsession)):\n",
    "        p=test_session_data[m] \n",
    "        line={\"uid\": p[0],\"impression\": {},\"time\":p[2]}\n",
    "        for j in range(len(predictsession[m])):\n",
    "            line[\"impression\"][p[1][j]]=float(predictsession[m][j])\n",
    "        f.write(JSONEncoder().encode(line)+'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
